---
layout: page
---

{% comment %}

	Editorial by Pixelarity
	pixelarity.com | hello@pixelarity.com
	License: pixelarity.com/license

	Name:

		Generic page

	Description:

		A generic content page.

{% endcomment %}

<header class="main">
	<h1>Research Projects</h1>
</header>


<p>We realize energy-efficient computing systems in the <span style="font-style: italic;color: #DC4405">hardware and algorithm co-design</span> fashion. Our vision of <span style="font-style: italic;color: #DC4405">end-to-end</span> hardware-software convergence includes research projects that: (1) simulate, fabricate, and characterize novel electronic devices that natively possess functions directly applicable to AI applications in their dynamics, (2) develop device physics-enabled algorithms for brain-inspired neuromorphic computing systems and AI accelerators, and (3) Integrate the devices and algorithms with customized circuits and architecture in a chip.</p>
<p> Maximizing each counterpart's capability, the co-design manner will meet an urgent demand for energy-efficiency deteriorated by ever-growing data size and algorithmic complexity for better AI while achieving high performance. Smartly leveraging device dynamics will eliminate unnecessary costs arising from digital-processing of analog and non-linear functions that are natural to human but expensive to digital computers. Inspiration from the brain grants abilities to learn and perform intelligent tasks better and more efficiently in a biological manner to build intelligent systems, without massive astronomical training and inference costs. All the elements can collaboratively accomplish the dream of true “Artificial Intelligence” that thinks, learns, and operates as the name means, beyond the current stage of “machine intelligence”.</p>


<hr class="major" />
<span class="image main", id="pnn"><img src="{{ site.baseurl }}/images/PNN.png" alt="" /></span>
<h2>Physics-based Neural Network (PNN) as a new type of deep learning</h2>
<p>Neural networks (NNs) have been inspired by the biological computing core, the brain. The inspiration makes NNs employ <span style="font-style: italic;color: #DC4405">non-linear</span> behavior of biological neurons and synapses which provide higher-order information processing capability to the brain. For this reason, today's NNs leverage mathematical non-linear functions in every hierarchy which are expensive in energy and time. <span style="font-style: italic;color: #DC4405">Physics-based NNs (PNNs)</span> have been investigated to ease the computational burdens that are inevitable for conventional non-linear functions. We do research hyper-dimensional computing, <span style="font-style: italic;color: #DC4405">reservoir computing</span> (RC), enabled by memristor internal physics. In the RC, the memristor plays the role of <span style="font-style: italic;color: #DC4405">natural non-linear function</span> with superior energy-efficiency as well as performance on specific tasks. For advanced RC systems and PNNs, we attack multiple live issues such as deep PNNs, backpropagation training through black-boxed physical systems, and expansion of PNNs to various AI tasks.</p>

<span style="font-weight: bold; font-style: italic; font-size: 1.0em;">Related Papers </span> </a> <br/>
<a href="https://www.nature.com/articles/s41928-024-01169-1"><span style="font-weight: bold; font-size: 0.8em;">Efficient Data Processing Enabled by Tunable Dynamics in Entropy-Stabilized Oxide Memristors </span> </a>, <span style="font-style: italic;">Nature Electronics</span>, 7, 466, (2024) <br/>
<a href="https://onlinelibrary.wiley.com/doi/full/10.1002/aisy.202400265"><span style="font-weight: bold; font-size: 0.8em;">RN-Net: Reservoir Nodes-Enabled Neuromorphic Vision Sensing Network </span> </a> <br/>

<hr class="major" />
<span class="image main", id="cim"><img src="{{ site.baseurl }}/images/CIM.png" alt="" /></span>
<h2>Compute-in-Memory (CiM) with emerging memories</h2>
<p>The way to solve problems and execute tasks of the human has been revolutionized by the emergence of AI. However, the demand for the better has made it more complicated and power-consuming with the ever-growing amount of information. To accelerate the process while reducing energy consumption, brain-inspired computing has been developed. Compute-in-memory (CiM) is one of the themes and has shown promise through accelerating multiply-and-accumulate (MAC) operations. It eliminates the data movement between computing and memory units that are segregated in Von-Neumann architecture, leading to faster and more energy-efficient operations. We co-design CiM-based computing systems, such as AI accelerators and Ising Machines, with novel memory technologies, such as resistive random-access memory (RRAM), Ferroelectric RAM (FeRAM), and Ferroelectric field effect transistor (FeFET). As a part of Co-design, CMOS circuits and computer architecture are customized to make such memories work properly according to the purpose.</p>
